{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "#### Playing around.\n",
    "http://radimrehurek.com/gensim/models/word2vec.html  \n",
    "http://rare-technologies.com/word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Installed Gensim using pip (pip3 install gensim)\n",
    "import gensim, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "#Take text as input\n",
    "\n",
    "# good song: https://www.youtube.com/watch?v=ZI5Xpl7H5O4\n",
    "sentences = [\n",
    "            \"When we were younger we thought\",\n",
    "            \"Everyone was on our side\",\n",
    "            \"Then we grew a little bit\",\n",
    "            \"And romanticized the time I saw\",\n",
    "            \"Flowers in your hair\",\n",
    "            \"It takes a boy to live\",\n",
    "            \"It takes a man to pretend he was there\",\n",
    "            \"So then we grew a little and knew a lot\",\n",
    "            \"And now we demonstrated it to the cops\",\n",
    "            \"And all the things we said\",\n",
    "            \"We were self-assured\",\n",
    "            \"Cause it's a long road to wisdom\",\n",
    "            \"But it's a short one\",\n",
    "            \"To being ignored\",\n",
    "            \"Be in my eyes\",\n",
    "            \"Be in my heart\",\n",
    "            \"Be in my eyes ai yai yai\",\n",
    "            \"And be in my heart\",\n",
    "            \"So now I think that I could\",\n",
    "            \"Love you back\",\n",
    "            \"And I hope it's not too late cause you're so attractive\",\n",
    "            \"And the way you move\",\n",
    "            \"I won't close my eyes\",\n",
    "            \"It takes a man to live\",\n",
    "            \"It takes a woman to make him compromise\",\n",
    "            \"Be in my eyes\",\n",
    "            \"Be in my heart\",\n",
    "            \"Be in my eyes ai yai yai\",\n",
    "            \"And be in my heart\"\n",
    "            ]\n",
    "\n",
    "\n",
    "text_as_input = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[  7.94550055e-04  -4.40386124e-03  -1.89173152e-03  -2.96877814e-04\n",
      "   4.37665451e-03  -1.86657533e-03  -1.93875985e-05   4.23460873e-03\n",
      "  -7.31863838e-04  -4.36010212e-03  -6.36437966e-04  -1.25557373e-04\n",
      "   2.85507715e-03   3.67421820e-03   8.33748840e-04   3.80591792e-03\n",
      "   1.88095248e-04  -1.80501956e-04   1.82600191e-03   4.73222136e-03\n",
      "   8.19584879e-04  -4.06967010e-03   6.59934129e-04   3.12638050e-03\n",
      "   1.59863580e-03  -8.07732402e-04   4.29852866e-03   4.48543206e-03\n",
      "   3.49671044e-03  -1.96162122e-03   4.97279223e-03  -5.69555152e-04\n",
      "  -3.78061412e-03  -2.68595875e-03   3.93522205e-03   1.13822683e-03\n",
      "   3.53659294e-03  -4.64660581e-04   5.81039116e-04  -1.41581031e-03\n",
      "  -4.24917275e-03   1.80743041e-03  -3.63670685e-03   4.77028359e-03\n",
      "   9.01320775e-04  -2.15667137e-03  -1.69150706e-03  -2.03129975e-03\n",
      "   2.85886088e-03  -4.05875593e-03  -4.27962933e-03   2.96198647e-03\n",
      "  -3.61326314e-03   4.79324860e-03   2.17763567e-03   3.06151458e-03\n",
      "   3.60754062e-03   4.60368767e-03   2.20685592e-03   3.58579168e-03\n",
      "  -1.10235368e-03  -1.65774906e-03   1.33649271e-03  -3.93653335e-03\n",
      "  -8.35894956e-04   4.28807037e-03  -2.63162493e-03  -3.27280164e-03\n",
      "   3.57388170e-03   1.67323707e-03   1.73322461e-03  -2.43108440e-03\n",
      "   3.62125505e-03   3.85681796e-03   1.19923579e-03   2.65816692e-03\n",
      "  -2.98699643e-03  -3.75046604e-03  -6.90946355e-04   4.24276432e-03\n",
      "  -1.09286571e-03  -4.87504620e-03   2.61638942e-03  -6.42372819e-04\n",
      "   3.27160070e-03  -1.76381669e-03   1.77236705e-03  -3.49955284e-03\n",
      "   1.06832056e-04   1.71265029e-03   2.20361468e-03   9.49325564e-04\n",
      "  -2.04277807e-03  -2.13992945e-03   1.29702687e-03   4.22688713e-03\n",
      "   2.85584945e-03   2.64312665e-04   9.03312583e-04  -3.91784962e-03]\n"
     ]
    }
   ],
   "source": [
    "# Produce word vectors as output\n",
    "\n",
    "print(len(model['eyes']))\n",
    "print(model['eyes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0859880426237\n",
      "[('short', 0.21470236778259277), ('all', 0.16355571150779724), ('but', 0.1589709222316742), ('lot', 0.14202375710010529), ('way', 0.13486923277378082), ('wisdom', 0.1290673464536667), ('attractive', 0.12276758253574371), ('so', 0.12208054214715958), ('were', 0.11758711189031601), ('saw', 0.11486733704805374)]\n"
     ]
    }
   ],
   "source": [
    "# use the model\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "#output looks quite rubbish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"using-100-dimensions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.084841960668\n",
      "[('short', 0.2155430018901825), ('all', 0.1628197282552719), ('but', 0.160665363073349), ('lot', 0.14193584024906158), ('way', 0.13456755876541138), ('wisdom', 0.12958869338035583), ('attractive', 0.12544073164463043), ('so', 0.12405821681022644), ('saw', 0.11690407246351242), ('were', 0.11499582231044769)]\n"
     ]
    }
   ],
   "source": [
    "# does using skip-grams (instead of cbow) help?\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1, sg=1)\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "# Nope. Other than the relative postion of saw and were, nothing changed much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.221442353363\n",
      "[('all', 0.4154433608055115), ('side', 0.33129364252090454), ('everyone', 0.32430994510650635), ('ai', 0.3072550296783447), ('romanticized', 0.3020595908164978), ('way', 0.29527682065963745), ('time', 0.2884533107280731), ('short', 0.2857748866081238), ('my', 0.2829861640930176), ('flowers', 0.2578805088996887)]\n"
     ]
    }
   ],
   "source": [
    "# does using fewer dimensions help? \n",
    "# Question : Between CBOW and Skip-grams, is there any one that is more sensitive to dimensionality?\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1, size=20)\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "# That changed things quite a bit\n",
    "# Questions: - Is the similarity measure a distribution (over what) that sums to 1? \n",
    "#            - What explains the increase of an order of magnitude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"using-20-dimensions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.363257447386\n",
      "[('all', 0.5712563991546631), ('flowers', 0.5606418251991272), ('self-assured', 0.49054253101348877), (\"won't\", 0.4330041706562042), ('ai', 0.4218556582927704), ('short', 0.39715641736984253), ('and', 0.3938533663749695), ('heart', 0.3632574677467346), ('way', 0.3517415225505829), ('your', 0.34322598576545715)]\n"
     ]
    }
   ],
   "source": [
    "# Even fewer dimensions:\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1, size=10)\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "# finally 'heart' appears in the top ten similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"using-10-dimensions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287160501645\n",
      "[('all', 0.5530124306678772), ('flowers', 0.5516061782836914), ('self-assured', 0.4808751940727234), ('ai', 0.42250221967697144), (\"won't\", 0.3722158372402191), ('way', 0.35233259201049805), ('your', 0.3382498025894165), ('short', 0.3333122134208679), ('then', 0.3264716565608978), ('and', 0.32210835814476013)]\n"
     ]
    }
   ],
   "source": [
    "# Interesting. Dimension 12 changes things quite a bit\n",
    "# Question: why?\n",
    "# To be honest, I don't think this is worth remarking over,\n",
    "# because there's so many parameters and so little data,\n",
    "# there's bound to be such fluctuations\n",
    "# Can you explain it though?\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1, size=12)\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "# 'heart' disappears from the top ten similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"using-12-dimensions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
