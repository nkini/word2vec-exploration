{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "#### Playing around.\n",
    "http://radimrehurek.com/gensim/models/word2vec.html  \n",
    "http://rare-technologies.com/word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Installed Gensim using pip (pip3 install gensim)\n",
    "import gensim, logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "#Take text as input\n",
    "\n",
    "# good song: https://www.youtube.com/watch?v=ZI5Xpl7H5O4\n",
    "sentences = [\n",
    "            \"When we were younger we thought\",\n",
    "            \"Everyone was on our side\",\n",
    "            \"Then we grew a little bit\",\n",
    "            \"And romanticized the time I saw\",\n",
    "            \"Flowers in your hair\",\n",
    "            \"It takes a boy to live\",\n",
    "            \"It takes a man to pretend he was there\",\n",
    "            \"So then we grew a little and knew a lot\",\n",
    "            \"And now we demonstrated it to the cops\",\n",
    "            \"And all the things we said\",\n",
    "            \"We were self-assured\",\n",
    "            \"Cause it's a long road to wisdom\",\n",
    "            \"But it's a short one\",\n",
    "            \"To being ignored\",\n",
    "            \"Be in my eyes\",\n",
    "            \"Be in my heart\",\n",
    "            \"Be in my eyes ai yai yai\",\n",
    "            \"And be in my heart\",\n",
    "            \"So now I think that I could\",\n",
    "            \"Love you back\",\n",
    "            \"And I hope it's not too late cause you're so attractive\",\n",
    "            \"And the way you move\",\n",
    "            \"I won't close my eyes\",\n",
    "            \"It takes a man to live\",\n",
    "            \"It takes a woman to make him compromise\",\n",
    "            \"Be in my eyes\",\n",
    "            \"Be in my heart\",\n",
    "            \"Be in my eyes ai yai yai\",\n",
    "            \"And be in my heart\"\n",
    "            ]\n",
    "\n",
    "\n",
    "text_as_input = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[  7.94550055e-04  -4.40386124e-03  -1.89173152e-03  -2.96877814e-04\n",
      "   4.37665451e-03  -1.86657533e-03  -1.93875985e-05   4.23460873e-03\n",
      "  -7.31863838e-04  -4.36010212e-03  -6.36437966e-04  -1.25557373e-04\n",
      "   2.85507715e-03   3.67421820e-03   8.33748840e-04   3.80591792e-03\n",
      "   1.88095248e-04  -1.80501956e-04   1.82600191e-03   4.73222136e-03\n",
      "   8.19584879e-04  -4.06967010e-03   6.59934129e-04   3.12638050e-03\n",
      "   1.59863580e-03  -8.07732402e-04   4.29852866e-03   4.48543206e-03\n",
      "   3.49671044e-03  -1.96162122e-03   4.97279223e-03  -5.69555152e-04\n",
      "  -3.78061412e-03  -2.68595875e-03   3.93522205e-03   1.13822683e-03\n",
      "   3.53659294e-03  -4.64660581e-04   5.81039116e-04  -1.41581031e-03\n",
      "  -4.24917275e-03   1.80743041e-03  -3.63670685e-03   4.77028359e-03\n",
      "   9.01320775e-04  -2.15667137e-03  -1.69150706e-03  -2.03129975e-03\n",
      "   2.85886088e-03  -4.05875593e-03  -4.27962933e-03   2.96198647e-03\n",
      "  -3.61326314e-03   4.79324860e-03   2.17763567e-03   3.06151458e-03\n",
      "   3.60754062e-03   4.60368767e-03   2.20685592e-03   3.58579168e-03\n",
      "  -1.10235368e-03  -1.65774906e-03   1.33649271e-03  -3.93653335e-03\n",
      "  -8.35894956e-04   4.28807037e-03  -2.63162493e-03  -3.27280164e-03\n",
      "   3.57388170e-03   1.67323707e-03   1.73322461e-03  -2.43108440e-03\n",
      "   3.62125505e-03   3.85681796e-03   1.19923579e-03   2.65816692e-03\n",
      "  -2.98699643e-03  -3.75046604e-03  -6.90946355e-04   4.24276432e-03\n",
      "  -1.09286571e-03  -4.87504620e-03   2.61638942e-03  -6.42372819e-04\n",
      "   3.27160070e-03  -1.76381669e-03   1.77236705e-03  -3.49955284e-03\n",
      "   1.06832056e-04   1.71265029e-03   2.20361468e-03   9.49325564e-04\n",
      "  -2.04277807e-03  -2.13992945e-03   1.29702687e-03   4.22688713e-03\n",
      "   2.85584945e-03   2.64312665e-04   9.03312583e-04  -3.91784962e-03]\n"
     ]
    }
   ],
   "source": [
    "# Produce word vectors as output\n",
    "\n",
    "print(len(model['eyes']))\n",
    "print(model['eyes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0859880426237\n",
      "[('short', 0.21470236778259277), ('all', 0.16355571150779724), ('but', 0.1589709222316742), ('lot', 0.14202375710010529), ('way', 0.13486923277378082), ('wisdom', 0.1290673464536667), ('attractive', 0.12276758253574371), ('so', 0.12208054214715958), ('were', 0.11758711189031601), ('saw', 0.11486733704805374)]\n"
     ]
    }
   ],
   "source": [
    "# use the model\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "#output looks quite rubbish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"using-100-dimensions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.084841960668\n",
      "[('short', 0.2155430018901825), ('all', 0.1628197282552719), ('but', 0.160665363073349), ('lot', 0.14193584024906158), ('way', 0.13456755876541138), ('wisdom', 0.12958869338035583), ('attractive', 0.12544073164463043), ('so', 0.12405821681022644), ('saw', 0.11690407246351242), ('were', 0.11499582231044769)]\n"
     ]
    }
   ],
   "source": [
    "# does using skip-grams (instead of cbow) help?\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1, sg=1)\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "# Nope. Other than the relative postion of saw and were, nothing changed much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.221442353363\n",
      "[('all', 0.4154433608055115), ('side', 0.33129364252090454), ('everyone', 0.32430994510650635), ('ai', 0.3072550296783447), ('romanticized', 0.3020595908164978), ('way', 0.29527682065963745), ('time', 0.2884533107280731), ('short', 0.2857748866081238), ('my', 0.2829861640930176), ('flowers', 0.2578805088996887)]\n"
     ]
    }
   ],
   "source": [
    "# does using fewer dimensions help? \n",
    "# Question : Between CBOW and Skip-grams, is there any one that is more sensitive to dimensionality?\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1, size=20)\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "# That changed things quite a bit\n",
    "# Questions: - Is the similarity measure a distribution (over what) that sums to 1? \n",
    "#            - What explains the increase of an order of magnitude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"using-20-dimensions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.363257447386\n",
      "[('all', 0.5712563991546631), ('flowers', 0.5606418251991272), ('self-assured', 0.49054253101348877), (\"won't\", 0.4330041706562042), ('ai', 0.4218556582927704), ('short', 0.39715641736984253), ('and', 0.3938533663749695), ('heart', 0.3632574677467346), ('way', 0.3517415225505829), ('your', 0.34322598576545715)]\n"
     ]
    }
   ],
   "source": [
    "# Even fewer dimensions:\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1, size=10)\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "# finally 'heart' appears in the top ten similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"using-10-dimensions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287160501645\n",
      "[('all', 0.5530124306678772), ('flowers', 0.5516061782836914), ('self-assured', 0.4808751940727234), ('ai', 0.42250221967697144), (\"won't\", 0.3722158372402191), ('way', 0.35233259201049805), ('your', 0.3382498025894165), ('short', 0.3333122134208679), ('then', 0.3264716565608978), ('and', 0.32210835814476013)]\n"
     ]
    }
   ],
   "source": [
    "# Interesting. Dimension 12 changes things quite a bit\n",
    "# Question: why?\n",
    "# To be honest, I don't think this is worth remarking over,\n",
    "# because there's so many parameters and so little data,\n",
    "# there's bound to be such fluctuations\n",
    "# Can you explain it though?\n",
    "model = gensim.models.Word2Vec(text_as_input, min_count=1, size=12)\n",
    "print(model.similarity('heart', 'eyes'))\n",
    "print(model.most_similar('eyes'))\n",
    "# 'heart' disappears from the top ten similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"using-12-dimensions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using existing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://mattmahoney.net/dc/textdata\n",
    "# (June 11, 2006) Abstract: The entropy of \"clean\" written English, \n",
    "# in a 27 character alphabet containing only the letters a-z \n",
    "# and nonconsecutive spaces, has been estimated to be between\n",
    "# 0.6 and 1.3 bits per character [3,8]. \n",
    "# We find that most of the best compressors will compress Wikipedia text \n",
    "# (enwik9, 1 GB) and equivalent cleaned text (fil9, 715 MB) to about the same ratio, \n",
    "# usually within 3% of each other. Low end compressors will compress clean text about 5% smaller. \n",
    "# Furthermore, a quick test on 100 MB of cleaned text (text8) will predict \n",
    "# a compression ratio that is about 2% to 4% below the true ratio on fil9 for most compressors.\n",
    "\n",
    "input_as_text = gensim.models.word2vec.Text8Corpus('text8')\n",
    "model = gensim.models.Word2Vec(input_as_text)\n",
    "model.save('text8.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71290"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(model.vocab))\n",
    "print(model['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7076966166496277),\n",
       " ('princess', 0.6495625972747803),\n",
       " ('empress', 0.6394662857055664),\n",
       " ('elizabeth', 0.619990348815918),\n",
       " ('throne', 0.6175369620323181),\n",
       " ('mary', 0.6152124404907227),\n",
       " ('prince', 0.6120865345001221),\n",
       " ('son', 0.6086326241493225),\n",
       " ('sigismund', 0.600396454334259),\n",
       " ('regent', 0.5966161489486694)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "most_similar(positive=[], negative=[], topn=10, restrict_vocab=None)\n",
    " \n",
    "    Find the top-N most similar words. \n",
    "    Positive words contribute positively towards the similarity, negative words negatively.\n",
    "\n",
    "    This method computes cosine similarity between \n",
    "    a simple mean of the projection weight vectors of the given words\n",
    "    and the vectors for each word in the model. \n",
    "    The method corresponds to the word-analogy and distance scripts in the original word2vec implementation.\n",
    "\n",
    "    If topn is False, most_similar returns the vector of similarity scores.\n",
    "'''\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Which word from the given list doesn’t go with the others?\n",
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74817574610712323"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute cosine similarity between two words.\n",
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "most_similar_cosmul(positive=[], negative=[], topn=10)\n",
    "\n",
    "    Find the top-N most similar words, using the multiplicative combination objective proposed by Omer Levy and Yoav Goldberg in [4]. Positive words still contribute positively towards the similarity, negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
    "\n",
    "    In the common analogy-solving case, of two positive and one negative examples, this method is equivalent to the “3CosMul” objective (equation (4)) of Levy and Goldberg.\n",
    "\n",
    "    Additional positive or negative examples contribute to the numerator or denominator, respectively – a potentially sensible but untested extension of the method. (With a single positive example, rankings will be the same as in the default most_similar.)\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> trained_model.most_similar_cosmul(positive=['baghdad', 'england'], negative=['london'])\n",
    "    [(u'iraq', 0.8488819003105164), ...]\n",
    "\n",
    "    [4]\tOmer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations, 2014.\n",
    "'''\n",
    "model.most_similar_cosmul(positive=['baghdad', 'england'], negative=['london'])\n",
    "'''\n",
    "[('sicily', 0.9835770130157471),\n",
    " ('persia', 0.9748784303665161),\n",
    " ('caliphate', 0.9729500412940979),\n",
    " ('tripoli', 0.9710833430290222),\n",
    " ('spain', 0.9706282019615173),\n",
    " ('arabs', 0.9693437218666077),\n",
    " ('italians', 0.9692755937576294),\n",
    " ('slavs', 0.9689947366714478),\n",
    " ('revolted', 0.9664160013198853),\n",
    " ('byzantines', 0.965680718421936)]\n",
    "'''\n",
    "model.most_similar(positive=['baghdad', 'england'], negative=['london'])\n",
    "'''\n",
    "[('tripoli', 0.682729959487915),\n",
    " ('sicily', 0.6808061599731445),\n",
    " ('persia', 0.6475439071655273),\n",
    " ('spain', 0.640567421913147),\n",
    " ('syria', 0.6323617100715637),\n",
    " ('caliphate', 0.6298861503601074),\n",
    " ('ethiopia', 0.6293022632598877),\n",
    " ('gaul', 0.6287931203842163),\n",
    " ('carthage', 0.6280422806739807),\n",
    " ('damascus', 0.6251910328865051)]\n",
    "'''\n",
    "# Huh?! Interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesser known (?) applications using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pentagonal ssb ac china tonalsoft leap ma house pressures consultant \n"
     ]
    }
   ],
   "source": [
    "# Generative\n",
    "# Generating a random sentence.\n",
    "# I think that an N-gram Language model would be better at this sort of generation though.\n",
    "'''\n",
    "import numpy as np\n",
    "model_word_vector = np.array( my_vector, dtype='f')\n",
    "topn = 20;\n",
    "most_similar_words = model.most_similar( [ model_word_vector ], [], topn)\n",
    "\n",
    ">>> your_word_vector = array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32)\n",
    ">>> model.most_similar(positive=[your_word_vector], topn=1))\n",
    "'''\n",
    "random_sentence = \"\"\n",
    "for i in range(10):\n",
    "    random_word_vector = model.seeded_vector(random_sentence)\n",
    "    model_word_vector = np.array( random_word_vector, dtype='f')\n",
    "    word = model.most_similar(positive=[model_word_vector], topn=1)[0][0]\n",
    "    random_sentence += word+\" \"\n",
    "print(random_sentence)\n",
    "# Output is not immediately impressive, \n",
    "# but the possibility of generating random stories with some post processing is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-155.42089844, -144.30784607, -130.59036255, -119.47730255], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " score(sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
    "\n",
    "    Score the log probability for a sequence of sentences (can be a once-only generator stream). \n",
    "    Each sentence must be a list of unicode strings. \n",
    "    This does not change the fitted model in any way (see Word2Vec.train() for that)\n",
    "\n",
    "    Note that you should specify total_sentences; \n",
    "    we’ll run into problems if you ask to score more than this number of sentences \n",
    "    but it is inefficient to set the value too high.\n",
    "\n",
    "    See the article by [taddy] and the gensim demo at [deepir] for examples of \n",
    "    how to use such scores in document classification.\n",
    "    [taddy]\tTaddy, Matt. Document Classification by Inversion of Distributed Language Representations, \n",
    "            in Proceedings of the 2015 Conference of the Association of Computational Linguistics.\n",
    "    [deepir]\thttps://github.com/TaddyLab/gensim/blob/deepir/docs/notebooks/deepir.ipynb\n",
    "'''\n",
    "\n",
    "# Note that this requires the model to have used hierarchical softmax, not negative sampling\n",
    "model_hs = gensim.models.Word2Vec(input_as_text, hs=1)\n",
    "model_hs.save('text8.hs.model')\n",
    "\n",
    "# Maybe this could find an application in Question Answering?\n",
    "# It rained on a hot summer afternoon and a puddle formed. \n",
    "# After several hours, the puddle was gone. Which two processes made the puddle form and then disappear?     \n",
    "#   (A) precipitation followed by evaporation     \n",
    "#   (B) deposition followed by evaporation     \n",
    "#   (C) precipitation followed by runoff     \n",
    "#   (D) deposition followed by runoff\n",
    "sentences = [\"precipitation followed by evaporation\",\n",
    "             \"deposition followed by evaporation\",\n",
    "             \"precipitation followed by runoff\",\n",
    "             \"deposition followed by runoff\"]\n",
    "\n",
    "model_hs.score(sentences)\n",
    "# How do you incorporate the 'facts' in the question into the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "n_similarity(ws1, ws2)\n",
    "    Compute cosine similarity between two sets of words.\n",
    "'''\n",
    "model.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])   #0.61540466561049689 in gensim's docs\n",
    "model.n_similarity(['restaurant', 'japanese'], ['japanese', 'restaurant'])   #nearly 1\n",
    "model.n_similarity(['sushi'], ['restaurant']) == model.similarity('sushi', 'restaurant')     #True\n",
    "# Try to get something like the following to work\n",
    "# model.n_similarity(['Dravid','plays', India], ['Bolt','runs','Jamaica'])\n",
    "# model.n_similarity(['Wodehouse','Author','Jeeves'],['Brando','Actor','Godfather'])\n",
    "# Can you think of an example for the above that will likely work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'india'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rome - Italy = Beijing - China\n",
    "vector = model['rome'] - model['italy'] + model['china']\n",
    "vector = model['india'] - model['bombay'] + model['usa']\n",
    "model.most_similar(positive=[vector], topn=1)[0][0]\n",
    "#why is this not working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "What do the following do?\n",
    "\n",
    "gensim.models.word2vec.score_cbow_pair(model, word, word2_indices, l1)\n",
    "\n",
    "gensim.models.word2vec.score_sg_pair(model, word, word2)\n",
    "\n",
    "gensim.models.word2vec.train_cbow_pair(model, word, input_word_indices, l1, alpha, learn_vectors=True, learn_hidden=True)\n",
    "\n",
    "gensim.models.word2vec.train_sg_pair(model, word, context_index, alpha, learn_vectors=True, learn_hidden=True, context_vectors=None, context_locks=None)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
